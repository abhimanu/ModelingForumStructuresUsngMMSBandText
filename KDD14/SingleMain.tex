\documentclass{sig-alternate}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{times}
\usepackage{microtype}
\usepackage{enumitem}
% \usepackage{graphicx, subfigure, fink, grffile, placeins}
\usepackage{hyperref}
\usepackage{color}
\usepackage{url}
\usepackage{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}


 \usepackage{upgreek}
% \input{dfn.tex}
\newcommand{\ericx}[1]{\textcolor{red}{\\ eric-comment: #1}}
\newcommand{\abhi}[1]{\textcolor{blue}{\\ abhi-comment: #1}}

\newcommand{\order}[1]{\textit{O}(#1)}
\newcommand{\comment}[1]{\textcolor{red}{[#1]}}

\title{Large Scale Structure Aware Community Discovery in Online Forums}
\numberofauthors{6}
\author{
\alignauthor
Abhimanu Kumar \\
\email{abhimank@cs.cmu.edu}
\alignauthor
Shriphani Palakodety \\
\email{shriphanip@gmail.com}
\alignauthor
Chong Wang \\
\email{chongw@cs.cmu.edu}
\and
\alignauthor
Miaomiao Wen\\
\email{mwen@cs.cmu.edu}
\alignauthor
Carolyn P. Rose\\
\email{cprose@cs.cmu.edu}
\alignauthor
Eric P. Xing\\
\email{epxing@cs.cmu.edu}
\sharedaffiliation
\affaddr{School of Computer Science}  \\
\affaddr{Carnegie Mellon University}  \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
%\newcommand{\comment}[1]{{\color{red}{#1}}}

%\nipsfinalcopy

\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}

\begin{document}
\maketitle
\begin{abstract}
Online discussion forums are complex webs of overlapping communities. 
Users choose to participate in threads according to their interests, 
whether fleeting or more persistent. Thus, the network structure left 
behind as their behavior trace can be seen as encoding evidence of 
coordinated interests and values shared within sub-communities that 
members move among from time to time.  We propose a latent 
hierarchical model to incorporate this structure that leverages topic 
models (LDA) along with mixed membership stochastic block (MMSB) 
models. This makes the model complex and so both traditional sampling and 
variational based inference are very slow to converge on it. We overcome 
this by proposing a new parallel inference scheme based on stochastic 
variational inference that is highly scalable. We evaluate our 
model on three large-scale datasets, which we refer to as 
Cancer-ThreadStarter (22K users and 14.4K threads), Cancer-NameMention(15.1K 
users and 12.4K threads) and StackOverFlow (1.19 million users and 
4.55 million threads). Qualitatively, we demonstrate that our 
model uncovers new communities that were hitherto invisible to  
simpler MMSB and spectral clustering based methods. Quantitatively, we 
show that our model performs significantly better than MMSB, LDA, 
matrix factorization and a bag of words model in predicting 
user reply structure within threads. In addition, we demonstrate 
via rigorous synthetic data experiments that the proposed active sub-network 
discovery model is stable and recovers the original parameters of 
the experimental setup with high probability. We also provide speed 
comparisons of our model with varying degrees of 
parallelism and data sizes.  

\end{abstract}


% NOTE: I make the argument that Poisson can be used to handle zero edges. and
% then I say that I use zero edges in Stack Overflow. I make sure to say that I am
% using equal proportion of those edges in training and test. Also I can take
% Prem's defense and say that I learn that param of weight.

\section{Introduction}
Online forums provide a digital representation of the intangible bonds that are more frequently the object of study of social scientists who study how communities form, maintain themselves, and provide opportunities for peripheral or core participation.
Online communities provide a richly structured environment in which users may choose to affiliate with alternative subcommunities, and in each case, may engage in different topics of discussion or use different communication styles.  Individual choices to start a new thread, or to participate in someone else's thread, add up to an intricate emerging subcommunity structure, where each subcommunity develops its own norms of communication in terms of topic and style.  As researchers, we have the opportunity to uncover this resulting structure.  The behavior signature of a user in terms of distribution of topic and style choices may thus diverge locally within a subcommunity from the distribution one might compute over the whole history of this user's participation.  Thinking within a topic modeling framework, we can think of this as a local versus global topic distribution for a user.  In this paper, we present a model designed to tease apart the local topic distributions by leveraging the evidence of subcommunity structure provided by the reply links in discussion threads.  Using this model, we are able to uncover the intricate subcommunity structure of a discussion forum by integrating hints of this structure that are found in a complementary way in the text as well as the network within the online community behavior trace data.

We build on a long history of analysis of online social networks and discussion forums.  Some prior work focuses entirely on the network data ~\cite{Shi:2000:NCI:351581.351611,
Shi00learningsegmentation}.  However, recently there has been interested in developing probabilistic 
graphical models~\cite{ Airoldi:2008:MMS:1390681.1442798} that are able to combine network \& text 
~\cite{Ho:2012:DHT:2187836.2187936,Nallapati:2008:JLT:1401890.1401957}. These earlier attempts are limited in that they have not yet taken advantage of the dynamics of sub-networks and the related thread-based framework
within which forum discussions take place. In our work, we apply concepts that have proven their value in computational biology where active sub-network modelling has been used to model sub-networks
of gene interactions~\cite{journals/ploscb/DeshpandeSVHM10,Lichtenstein:Charleston}. 

Taking into account sub-network interaction
dynamics is important to correctly model participant behavior as users are free to move between emerging sub-communities over their history of participation. For
example, in an online medical support community, as a user's disease progresses, or that of other participants that user has become friends with, that user may be interested in different issues and therefore participate in different discussions.  In this way, users may participate in different roles, or modes of participation, such as being simultaneously a cancer survivor and a caregiver to someone else who has cancer. In an community like Stack Overflow, users may participate as novices for one programming language while also participating as experts of another.  Without the ability to separate these modes of participation, the model could easily be confused by inconsistent participation patterns either in terms of the reply link structure, or word distributions, or both.  A model that can tease apart the different modes of participation should be able to fit the data more cleanly.

The aim of our work is to provide novel insights into the ways users move between modes of participation in online forum discussions. Our proposed approach contrasts with existing techniques built on  
simple aggregation of reply networks into a single graph and user 
text across subforums into a single document per user.  This simplistic approach makes an invalid assumption about consistency of user behavior and can thus cause severe loss of information. 
In contrast, we show that modeling the thread structure of the forum in a way that is tightly integrated with the text leads to discovery of new sub-communities that were overlooked by simple
aggregated-network analysis methods such as spectral learning, MMSB, etc. Our approach also 
provides better user-user link prediction results. 
The proposed model incurs a heavy computational complexity due to a non-trivial network-text interplay at both latent variable and parameter levels.  To address this issue, we present a highly scalable procedure based on stochastic variational inference that is able to handle millions of users with billions of threads in matter of hours. We believe our proposed approach is practical for analyzing real world forums, as we demonstrated in our experiments. Below, we highlight the specific challenges we have successfuly addressed in our work, and the main technical contributions we have made.

\vspace*{-0.5\baselineskip}
\paragraph{Challenges} A major computational challenge of this work has been the ability to model
active sub-networks in a large forum with millions of users and
threads.  Modelling the meaningful integration of text and network structure together such that it provides
quantifiable improvments over either separately is the next major challenge.  In our case, the network structure includes both the separation of data into threads as well as the reply structure within threads.
In order to model the graded quality of strong and weak ties within a community structure, we model weighted edges instead of binary edges as is the case in the traditional MMSB model. However, this adds to the 
complexity of the model and makes single-processor inference challenging. Thus, we have been faced with the challenge of developing a novel, highly scalable inference method.  As we will demonstrate, we have successfully met each of these challenges in our work.

\vspace*{-0.5\baselineskip}
\paragraph{Contributions}
\vspace*{-0.5\baselineskip}
\begin{itemize}
\itemsep0em
  \item By directly modeling active sub-networks, this work provides new insights into how information about varying interactions across threads amongst users can lead to discovery of novel communities that were ignored by less-sophisticated methods such as spectral clustering and MMSB.
 \item By combining the text and network along with thread structure in the active sub-networks, our method is able to outperform LDA, MMSB, Matrix Factorization and a bag of words model in link prediction on all three datasets we studied.
\item We developed a highly scalable inference algorithm for our proposed model, which is able to achieve convergence in
matter of hours for users and threads that are in order of a million despite the time complexity of \order{users$\times$users$\times$threads}, and we validated correctness of our inference algorithm on synthetic experiments on consistency and stability in model recovery. In particular, we derive a scalable inference approach based on
stochastic variational inference (SVI) with sub-sampling~\cite{Hoffman:2013:SVI} 
that has the capacity to deal with this massive scale data and parameter space.  While the graded edge weighting we employ adds complexity to the model, we are also able to creatively leverage its properties to enhance our scalability.  In particular, a Poisson based scheme need not model zero edges
(\cite{Kerrer:Newman}), where as MMSB style approaches~\cite{Airoldi:2008:MMS:1390681.1442798}
 must explicitly model them.
A further effort of parallelization in the inner optimization loops of local variational parameters further enhances the efficiency of our approach. To our knowledge, this work represents the largest 
model-based effort for analyzing social graph data that takes into account both user content and emerging thread structure
\end{itemize} 
\vspace*{-1\baselineskip}

%\ericx{Here is a comment: you are using the word "model" in very inaccurate way all over the place. What you really meant can be the graphical model, the algorithm you devised to solve the inference/learning problem with the model, or just the overall method/approach we propose here. I have corrected such confusion in the intro, but please be careful in the main text.}


% \section{Graphical Model \& Generative Story}
% \section{Approach}
\section{Forum Structure Modeling}
\label{sec:approach}
Online forums have a threaded discussion structure that is used to separate discussions pertaining to different concerns, or interesting to different subsets of users.  Thus, without some explicit leveraging of this aspect of structure, important social context would be lost.  We begin here by offering our conceptual characterization of the structure we seek to leverage in our graphical model.  We then present the generative story of the model.  And finally, we discuss some specifics of terminology we will use throughout the remainder of the paper.

\subsection{Discussion in online forums}
Each thread within a discussion forum can be seen as conceptually linked with a mixture of subcommunities whose interests and values are represented within that discussion.  Thus, whenever two users interact in a thread, they do so assuming a particular mode of participation that contributes to that mixture of subcommunities.  In our model, each of those subcommunities is represented by a topic.  Thus, we refer to each topic  conceptually as a community.

More formally, when a user $u$ is participating in a thread associated with the interests and values of a community $c$, his personal connection 
with those values will be reflected in the extent to which his behavior is consistent with the associated
norms of behavior.  In other words, he tailors his post content accordingly. The interests and values of community $c$ are reflected through its associated topic's
word distribution.  By observing the topic distribution of user $u$ on thread $t$, we can get a sense of what role the user was playing on that thread.
In particular, what contribution that user made to the overall distribution of interests and values that were communicated during the discussion.  Threads are primarily group discussions.  We consider that it is important in a model to respect the extent to which each contribution is directed both at a specific user to whom user $u$ replies, but also the entirety of the set of users participating on the thread.  This detailed structure is not typically modelled, however, modeling this phenomenon brings the model closer to realities of online discussions.  We achieve this structuring in our graphical model by aggregating user posts within a thread, and modelling the extent to which the associated topic distribution was influenced by the other users participating on the thread.  We
 elaborate on this in the
generative story of our model. 
% Another interesting property of such structured
% conversations is that there is an inherent bias towards the thread starter (or
% topic of the thread). Each user makes his contributions to the threaded
% discusison in light of this context. A model that takes this phenomena into
% account is closer to reality. 
% (\comment{This would be very relevant for post-and-response
% forums in our dataset such as Reddit and Stack Overflow. Right now our graphical
% model doesn't support this but it would be interesting to see how this can be brought in. 
% It might not be too difficult to do this}).
\subsection{Graphical model \& generative story}
\label{sec:gen-story}
Based on the description above our graphical model is designed as shown
in Figure~\ref{fig:finalThreadAggregationModel}. In this model
we aggregate the posts of 
a given user in a given thread $t$ into one document which has a token
set $N_{t,p}$ . This helps us incorporate the knowledge that a user's post is
influenced by the posts of the other users present on the thread, assuming that
he reads at least some of them.
The generative process for the model is as follows:
\begin{figure}
\centering
\includegraphics[height=4cm,width=8.5cm]{model.pdf}
\vspace*{-2\baselineskip}
\caption{\small{The proposed approach models active sub-network of users in the forum.
$U$ is the total number of user in all of the forum, $T$ is the number of
threads. $N_{T_p}$ is the total number of tokens user $p$ posted in thread $t$.
}}
\vspace*{-2\baselineskip}
\label{fig:finalThreadAggregationModel}
\end{figure}

%Assuming that there are total $N_t$ users in the thread $t$.  
% \begin{itemize}
%   \item For each Thread $t$,
\begin{itemize}[noitemsep,nolistsep]
  \item For each user $p $,
  \begin{itemize}[noitemsep,nolistsep]
    \item Draw a $K$ dimensional mixed membership vector 
    $\overset{\rightarrow}{\uppi}_{p} \sim$ Dirichlet($\alpha$). 
	This represents the community membership vector of the user $p$
	across $K$ communities.
  \end{itemize}
  \item For each topic pair $g$ and $h$,
    \begin{itemize}[noitemsep,nolistsep]
    \item Draw $B(g,h) \sim Gamma(\kappa,\eta)$; where $\kappa, \eta$ are
	parameters of the gamma distribution. $B_{g,h}$ provides the interaction
	strength between users of community $g$ and $h$.
  \end{itemize}

  \item For each pair of users $(p, q)$ and each thread $t$,
  \begin{itemize}[noitemsep,nolistsep]
    \item Draw membership indicator for the initiator, 
    $\overset{\rightarrow}{z}_{(p \rightarrow q,t)} \sim$
    Multinomial($\uppi_{p}$). 
	$\overset{\rightarrow}{z}_{(p \rightarrow q,t)}$ indicates
	the community user $p$ lies in when he initiates a conversation
	with user $q$ in thread $t$.
    \item Draw membership indicator for the receiver,
    $\overset{\rightarrow}{z}_{(p \leftarrow q,t)} \sim$
    Multinomial($\uppi_{q}$).
	$\overset{\rightarrow}{z}_{(p \leftarrow q,t)}$ indicates the
	community user $q$ lies in when he is interacting with user $p$
	in a conversation initiated by user $p$ in thread $t$.
    \item Sample the value of their interaction, $Y(p,q,t) \sim$
    Poisson( ${\overset{\rightarrow}{z}}^{\top}_{(p \rightarrow q,t)}
    B~\overset{\rightarrow}{z}_{(p \leftarrow q,t)}$ ). This is 
	the number of times user $p$ replies to user $q$ in thread $t$.
	\end{itemize}
	\item For each user $p \in t$,
	\begin{itemize}[noitemsep,nolistsep]
	  \item Form the set $\delta_{t,p}$ that contains all the users that p
	  interacts to on thread $t$,
	  \begin{itemize}[noitemsep,nolistsep]
	    \item For each word $w \in N_{t,p}$, 
	    \item Draw $z^{'}_{t,p,w}$ from $Dirichlet(\sum_{\forall q\in \delta_{t,p}} z_{(t,p
	    \rightarrow q)})$. This is the topic indicator for token $w$ of user's aggregated
		post in thread $t$.
	    \item Draw $w \sim \phi(w|\beta,z^{'}_{t,p,w}) $.
	  \end{itemize}
  \end{itemize}
\end{itemize}  
% \end{itemize}

The use of Poisson distribution for $Y(p,q,t)$  (the 
network edge between the user's $p$ and $q$) besides modelling non-binary edge
strength enables the model to ignore non-edges between users ($Y_{t,p,q}$) and
thus achieve faster convergence~\cite{Kerrer:Newman}. In MMSB style community block models, 
	 non-edges are to be modelled
explicitly.
\vspace*{-0.5\baselineskip}
%\begin{equation}
\small
\begin{align}
	&	\log L = \log \! P(Y, W, Z_{\leftarrow}, 
Z_{\rightarrow}, \Pi, B, \beta | \alpha, \eta, \theta, \alpha). \nonumber \\
      &= \sum_{t} \bigg[ \sum_{p,q} \! \log P(Y_{t,p, q} | Z_{t,p \rightarrow q} 
      , Z_{t,p \leftarrow q}, B) \nonumber  
      + \sum_{p,q} \log P(Z_{t, p \rightarrow q} | \Pi_q) \\ \nonumber 
      & + \sum_{p,q} \log \! P(Z_{t, p \leftarrow q} | \Pi_{q}) \bigg] 
%      + \sum_{p} \log \! P(\Pi_{p} | \alpha)  \\ \nonumber 
      + \bigg[ \sum_{t=1}^{T} \! \sum_{p \in t} \sum_{i=1}^{N_{T_{p}}} 
      \log \! P(w_{t,p,i} | Z'_{t,p,i}, \beta) \\ \nonumber 
      & + \sum_{t=1}^{T} \sum_{p \in t} \sum_{i=1^{N_{T_{p}}}} \log \! 
      P(Z'_{t,p,i} | \bar{Z}_{t, p \rightarrow q}) \bigg] + \sum_{k} \log P(\beta_{k} | 
      \eta)  
      \\ & +  \sum_{g,h} \log P(B_{g,h} | \kappa, \theta)
	  + \sum_{p} \log \! P(\Pi_{p} | \alpha)
 \label{eqn:LL}
\end{align}
%\end{equation}
\normalsize
The log-likelihood of the model described in figure \ref{fig:finalThreadAggregationModel} is given
 above.

\vspace*{-1\baselineskip}
\small
\begin{align}
	&q = \prod_{p}q(\Pi_{q} | \gamma_{p}) \prod_{t} \bigg[ \prod_{p, q} \! 
q(Z_{t, p \rightarrow q}, Z_{t, p \leftarrow q} | \phi_{t,p,q})  \nonumber\\ 
\cdot &\prod_{p \in t} \prod_{i=1}^{N_{T_{p}}} q(Z'_{t,p,i} | \chi_{t,p,i})
\bigg] %\nonumber \\
\cdot \prod_{g,h} q(B_{g,h} | \nu_{g,h} \lambda_{g,h}) \prod_{k} q(\beta{k} |
\tau_{k}).
\label{eqn:variationalQ}
\end{align}
\normalsize
We use variational approximation to maximize log-likelihood.
Equation~\ref{eqn:variationalQ} above is the approximation of the log-likelihood and we
use structured mean field~\cite{Xing_et_al:2003} to maximize parameters of $q$.
The local variational parameters, $\phi$ (MMSB parameters) and $\chi$ (LDA)
parameters, are maximized using equations \ref{eqn:phiUp} and \ref{eqn:chiUp}.
%are defined by equations ~\ref{eqn:phiDelta} and~\ref{eqn:chiDelta} respectively.
\small
\begin{align}
	\phi_{t,p,g,h} &\propto e^{\Delta_{\phi^{'}_{t,p,g,h}}}.
\label{eqn:phiUp}                                 \\
\chi_{t,p,i,k} &\propto e^{\Delta_{\chi^{'}_{t,p,g,h}}}.
\label{eqn:chiUp}
\end{align}
where $\Delta_{\phi^{'}_{t,p,g,h}}$ and $\Delta_{\chi^{'}_{t,p,g,h}}$ are
\begin{align}
\Delta_{\phi^{'}_{t,p,g,h}} &= y_{t,p,q}( \log \! \lambda_{g,h} + 
\Psi(\nu_{g,h})) - \nu_{g,h} \lambda_{g,h} - \log \! (y_{t,p,q}!)
\nonumber \\ & + \Psi(\gamma_{p,g}) - \Psi(\sum_{g} \gamma_{p,g})
%\nonumber \\  & 
+ \Psi(\gamma_{q,h}) - \Psi(\sum_{h} \gamma_{q,h})
\nonumber \\  & + \omega\sum_{i=1}^{N_{T_{P}}} \! \chi_{t,p,i,g} 
\bigg[ \ln \! \frac{\epsilon}{\delta_{p,t}} - \frac{1}{\delta_{t,p}} 
+ \ln \bigg( 1 + \frac{\epsilon}{\delta_{p,t}} \bigg) 
\cdot \frac{1}{\delta_{t,p}} \bigg].
\label{eqn:phiDelta}  \\
\Delta_{\chi^{'}_{t,p,g,h}} &= \bigg[ \Psi(\tau_{k,w_{t,p,i}}) - 
\Psi(\sum_{w_{t,p,i}} \! \tau_{k, w_{t,p,i}}) \bigg]
\nonumber \\  & + \ln \! (\frac{\epsilon}{\delta{p,t}}) \frac{1 - 
\sum_{q,h} \! \phi_{t,p,q,k,j}}{\delta_{t,p}} 
\nonumber\\  & + \frac{\sum_{q,h} \phi_{t,p,q,k,h}}{\delta_{t,p}} 
\ln \! (1 + \frac{\epsilon}{\delta_{t,p}}).
%\nonumber\\  & - 1 - \log \! \chi_{t,p,i,k}
\label{eqn:chiDelta}
\end{align}
\normalsize
and the traditional variational updates for global parameters $\gamma, \nu, \lambda$
(MMSB) and $\tau$ (LDA) are defined in the appendix.
%using equations
%\ref{eqn:gammaUp}, \ref{eqn:nuUp}, \ref{eqn:lambdaUp} and \ref{eqn:tauUp}
%respectively (details are in the appendix).
% ~\ref{sec:appendix}.
\vspace*{-0.5\baselineskip}
\subsection{Terminology} 
\label{sec:term}
We should distinguish between 
community, word topic and user roles. User role (or community topic) is the 
$\pi$ vector that we get from
the model (figure~\ref{fig:finalThreadAggregationModel}) that decides the 
membership proportion of a user in
different latent communities. Word topic is the $\beta$ vector of word topic
proportions from the LDA component of the model. There is a one-to-one
correspondence between $\beta$ and $\pi$ vectors as seen in
figure~\ref{fig:finalThreadAggregationModel}. $\beta$ helps us in identifying
what content users are generally interested in a given latent community.
% A user conversational role (or simply role) is closely related to 
%$\pi$. 
%$\pi$, as we know, is a community topic vector of length $K$
%that stores the membership probability of a user in each of the $K$
%communities. 
A user role can vary depending upon the 
context or thread of interaction of user $u$.
For example, for simplicity we may assume that $\pi$ vector's 
$k$-th coordinate is 1
and all else are 0 out of the total K coordinates, i.e. it predominantly relates
to the $k$-th latent community when the user $u$ interacts in thread $t$.
In this case the user role is predominantly in one single community.


\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c|c|}
 & users & threads & posts & edges\\\hline
 TS & 22,095 & 14,416 & 1,109,125 & 287,808\\\hline
 UM & 15,111 & 12,440 & 381,199 & 177,336\\\hline
 SO & 1,135,996 & 4,552,367 & 9,230,127 & 9,185,650\\\hline
\end{tabular}
\label{tab:dataStats}
\end{center}
\vspace*{-1\baselineskip}
\caption{Dataset statistics. SO mostly has edges with weight one.}
\vspace*{-1\baselineskip}
\end{table}




\section{Datasets}
\label{sec:dataset}

\begin{figure}
\begin{center}
%\includegraphics[height=6cm,width=9cm]{EdgeDistribution.pdf}
\includegraphics[height=6cm,width=9cm]{figs/datastats.pdf}
\end{center}
\vspace*{-2\baselineskip}
\caption{\small{Distribution of different edge weights over the 3 datasets. SO 
predominantly consists of edges with weight one. 
The label '11' contains edge weights of 11 and above.}}
\label{fig:EdgeDistribution}
\vspace*{-1.5\baselineskip}
\end{figure}
We analyse three real world datasets corresponding to two different forums: 1)
Cancer-ThreadStarter, 2) Cancer-UserName, and 3) Stack Overflow. To test the
stability of the model we use a synthetically generated dataset. 
The Cancer forum \footnote{\url{http://community.breastcancer.org}} 
is a self-help community where users who either have cancer, are 
concerned they may have cancer, or care for others who have cancer 
come to discuss their concerns and get advice and support.
StackOverflow is an
online forum for question answering primarily related to computer science. We
use the latest dump of Stack
Overflow~\footnote{\url{http://www.clearbits.net/torrents/2141-jun-2013}}. In
each of these datasets, when a user posts multiple times in a thread, all these
posts are aggregated into one document per thread as defined in
section~\ref{sec:gen-story}.
Number of times a user $u$ replies to user $v$ in thread $T$ is the edge weight
of edge $u\rightarrow v$ in thread $T$.
Table~\ref{tab:dataStats} gives the distributions of edges, posts, users and
threads in the three datasets used. 
\vspace*{-0.5\baselineskip}
\paragraph{Cancer-ThreadStarter (TS)} In the Cancer forum, the conversations happen
in a structured way where users post their responses on a thread by thread
basis. Every thread has a thread starter that posts the first message and
starts the conversation. 
We construct a graph from each thread by drawing a link from each participant on 
the thread to the thread starter and hence the name Cancer-ThreadStarter.
This graph has 22,095 users and 14,416 Threads. 
% \comment{Put number of
% posts and edges}
\vspace*{-0.5\baselineskip}
\paragraph{Cancer-Username Mention (UM)} Users mention each other by 
their usernames (or handle assigned to them in the
forum) while posting in many cases. We create a graph where in an edge between
user $u$ and user $v$ in thread $t$ means that user $u$ calls user $v$ by
username in thread $t$. This graph has 15,111 users and
12,440 threads.
% \comment{Put number of posts and edges}
\vspace*{-0.5\baselineskip}
\paragraph{Stack Overflow (SO)} In Stack Overflow users ask 
questions and then other users reply with their
answers. We obtain the ThreadStarter graph from this structure. This dataset has
1,135,996 users and 4,552,367 threads.
% \comment{Put number of posts and edges}
\vspace*{-0.5\baselineskip}
\paragraph{Synthetic data} We generate a synthetic dataset 
using the generative process defined in
section~\ref{sec:gen-story}. For stability experiments 
we fix users at 1000 and threads at 100. The number of 
posts and edges vary depending on the choice of priors $\alpha$ and $\eta$.
For scalability experiments we vary the number of user and threads.  
% \comment{Put number of posts and edges as well as the parameters}


\section{Scalable Estimation}
\label{estimation}

\begin{figure}
\begin{center}
\includegraphics[height=6cm,width=9cm]{figs/parallel_model_comparison.pdf}
\end{center}
\vspace*{-2\baselineskip}
\caption{\small{The negative log-likelihood vs incremental speed optimization routine. 
The inner smaller plot is a zoomed in version of the elbow section of the 
bigger plot. PSSV
(Parallel Sub-sampled Stochastic Variational), SSV(Sub-sampled Stochastic
Variational), SV(Stochastic Variational) and V(Variational). Each addition of
optimization increases the speed by several orders of magnitude. The final PSSV
is 4 times faster than (V)ariational and achieves a better log-likelihood too.
%\abhi{explain the left and right plot relation in detail}
}}
\label{fig:SpeedOptimization}
\vspace*{-1\baselineskip}
\end{figure}
The global update equations in the previous section are computationally
expensive, as we need to sum over all the updated local
variables. $U$ users with $T$ threads and vocabulary size $V$ leads to
$O(U^2T+UVT)$ local variables. Traditional sampling or 
variational estimation techniques would be prohibitively slow for such a model. In order to obtain faster convergence we
make use of stochastic variational approximation along with sub-sampling and parallelization. 
% Algorithm~\ref{algo:stochasticAlgo} describes in detail the sub-sampled SVI
% based approximate estimation of the model. 

The updates in case of SVI with sub-sampling follow a two step procedure. Step
one computes a local update for the global variables based on the sub-sampled
updated local variables. The local updates ($\gamma^{'},
\nu^{'}, \lambda^{'}$ and $\tau^{'}$) for the global variables ($\gamma,
\nu, \lambda$ and $\tau$) are

{\small
\begin{align}
\gamma_{p,k}^{'} &= \alpha_{k} + \frac{NT}{2|S_p|}\sum_{q \in S_p} \sum_{h}
\! \phi_{t,p,q,k,h} + \frac{NT}{2|S_p|}\sum_{q \in S_p} \! \sum_{g} \!
\phi_{t,q,p,g,k}. 
\label{eqn:gammaUpStoc}
\\
%\end{align}
%\vspace*{-1.5\baselineskip}
%\begin{align}
\nu_{g,h}^{'} &= \nu_{g,h}^{t}+\rho_\nu \frac{NT}{2|S_p|}\sum_{q \in
S_p}\frac{dL}{\partial\nu_{g,h}}.
\label{eqn:nuUpStoc}
%\end{align}
%\vspace*{-1.5\baselineskip}
%\begin{align}
\\
\lambda_{g,h}^{'} &= \frac{\bigg( \sum_{t} \! \sum_{p,q} \! \phi_{t,p,q,g,h}
y_{t,p,q} + \kappa_{g,h} \bigg) }{
 \bigg( \bigg( \sum_{t} \! \sum_{p,q} \! \phi_{t,p,q,g,h} \bigg) + 
\frac{1}{\theta_{g,h}} \bigg) \nu_{g,h}}.
\label{eqn:lambdaUpStoc}
%\end{align}
%\vspace*{-1.5\baselineskip}
%\begin{align}
\\
\tau_{p,v}^{'} &= \nu_{v} + \frac{NT}{2|S_p|} 
\bigg(\sum_{w_{t,p,i}=v}^{N_{t,p}} \chi_{t,p,i,k} \bigg),
\label{eqn:tauUpStoc}
\end{align} 
}
where $S_p$ is a set of neighborhood edges of user $p$, and $N$ and $T$ are
total number of edges and threads respectively in the network. The set $S_p$ is
chosen amongst the neighbors of $p$ by sampling an equal number of zero and non-zero
edges. 

\normalsize
\IncMargin{1em}

\begin{algorithm}[t]
\small
\SetAlgoLined
Input : $Y,W,c,\alpha,\theta,\kappa,\eta, S, U$\\
Initialize : $\gamma\leftarrow \gamma_0$,
$\tau\leftarrow \tau_0, \nu\leftarrow \nu_0, \lambda\leftarrow \lambda_0$\\
Equally divide all the threads in the forum into smaller sets $T_c$\\ 
\While{not converged}{
\For{$c$ processors \textbf{in parallel}}{
	pick $C_t$: a random subset of threads in $T_c$ of size $S$\\
	\For{each $t\in C_t$}{ 	
		pick $U_t$: a random subset of users in $t$ of size $U$ \\
		\For{each $p\in U_t$}{ 	
			$\forall q\in~neighborhood~\delta_{t,p}$\\
			\While{$\phi~\&~\chi$ not converged}{
				get new $\phi_{t,p\rightarrow q}$, $\phi_{t,p\leftarrow q}$,
				$\phi_{t,q\rightarrow p}$, $\phi_{t,q\leftarrow p}$\\
				and $\chi_{t,p,i} \forall i \in N_{t,p}$ \\
				iterate between $\phi$ and $\chi$ using
				equations~\ref{eqn:phiUp}~and~\ref{eqn:chiUp}			
			}
		}
	}
}
aggregate $\phi$ and $\chi$ obtained from different processors.\\
get local update $\gamma^{'}$,$\tau^{'}$, $\nu^{'}$, $\lambda^{'}$ via
stochastic approximation of traditional variational updates
(defined in appendix)\\
%equations~\ref{eqn:gammaUp},\ref{eqn:tauUp},\ref{eqn:nuUp},\ref{eqn:lambdaUp}.\\
get global updates of $\gamma$,$\tau$, $\nu$, $\lambda$; e.g. $\gamma^{t+1} =
(1-step)\gamma^{t}+(step)\gamma^{'}$ \\
Similarly globally update $\tau,\nu,\lambda$ as above using
equation~\ref{eqn:globalUpStoc}.
}
\label{algo:stochasticAlgo}
\caption{{\small Parallel Sub-sampling based Stochastic Variational (PSSV) inference for
the proposed model. Arguments $Y$ and $c$ are the input data and number of processors
respectively, whereas the rest of the 
arguments are parameters that are tuned over a heldout set.}}
\end{algorithm}

In step two of the sub-sampled SVI, the final update of global variable is
computed by the weighted average of the local updates of the global variable and
the variable's value in the previous iteration:
%{\small
\begin{align}
\mu^{t+1} = (1-\xi^t)\mu^{t} + \xi^t\mu^{'}. 
\label{eqn:globalUpStoc}
\end{align} 
\normalsize
%\vspace*{-1\baselineskip}
where $\mu$ represents any global variable from $\lambda, \nu, \gamma, \tau$.
 $\xi^t$ is chosen appropriately using the SGD literature and is
decreasing.  $\xi^t$ is standard stochastic
gradient descent rate at iteration $t$, also expressed as $\xi^t =
\frac{1}{(t+\zeta)^{\rho}}$~\cite{conf/nips/GopalanMGFB12}. $\zeta$ and
$\rho$ are set as 1,024 and 0.5 respectively for all our experiments in the
paper, and $t$ is the iteration number. 

We achieve further speed up by parallelizing the text ($\chi$) and network
($\phi$) local variational updates. This is achievable  as the
dependency between $\phi$ and $\chi$ parameters (defined in
equations~\ref{eqn:chiDelta} and \ref{eqn:phiDelta}) allows us to parallelize their
variational updates.
Algorithm~\ref{algo:stochasticAlgo} describes the parallelized SVI with
sub-sampling updates for the local parameters. This algorithm is 
based on the theoretical insights from~\cite{Hoffman:2013:SVI} and is 
quite stable as we will see in section~\ref{sec:results}. 
Figure~\ref{fig:SpeedOptimization} 
shows a plot of how the final (p)arallel (s)ub-sampling based (s)tochastic
(v)ariational (PSSV) inference is faster than each of its individual components.
The SO dataset described in section~\ref{sec:dataset} is used as the data for this 
experiment.
The number of parallel cores used in the PSSV scheme is four, whereas it's one
for the rest of the three.
The amount of sub-sampled forum threads is 400 and the total number of threads is 14,416.
All the schemes in the graph start with the same initialization values of the
hyper-parameters. PSSV is at least twice as fast as the nearest scheme and 
also obtains the best log-likelihood of all the four at the point of convergence.
The SV (stochastic variational) samples one thread at a time and therefore
takes some time in the beginning to start minimizing the objective value
(negative log-likelihood).
The objective value increases in the first few iterations for SV. The number of
iterations to be done by SV is very large, but each iteration takes the least time of
all four. The V (variational) scheme takes the smallest number of iterations to
converge, though its iterations are the most time consuming as it must go
through all the 14,416 threads in every iteration. 



\begin{figure}
\begin{center}
\includegraphics[height=6cm,width=9cm]{figs/data_ll.pdf}
\end{center}
\vspace*{-2\baselineskip}
\caption{The negative log-likelihood over heldout set for the fully tuned model on the
3 datsets}
\label{fig:finalLLheld}
%\vspace*{-1.5\baselineskip}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|}
 & $\alpha$ & $\omega$ & $\theta$ & $\kappa$ & $\eta$ & $K$\\\hline
 TS & 0.05 & 1e-4 & 2.5$\sim$1.5 & 2.5$\sim$1.5& 0.05 & 10\\\hline
 UM & 0.05 & 1e-3 & 2.0$\sim$1.0 & 2.0$\sim$1.0 & 0.05 & 10\\\hline
 SO & 0.05 & 1e-2 & 1.0$\sim$0.5 & 1.0$\sim$0.5 & 0.05 & 20\\\hline
\end{tabular}
\label{tab:tunedParameters}
\end{center}
\vspace*{-1\baselineskip}
\caption{Tuned values for the parameters. $\theta$ and $\kappa$ are matrices
and $a\sim b$ assigned to them means diagonal values are $a$ and non-diagonals
are $b$. $K$ is the number of topics}
\vspace*{-1\baselineskip}
\end{table}

\section{Experiments}

\subsection{Experimental Setup and Evaluation}
\label{sec:setup}
We divide each dataset into three subsets: 1) the training set, 2) the heldout set,
and 3) the test set. We learn our model on the training set and tune our priors 
($\alpha, \eta, \kappa, \theta$ etc.) on the heldout set. The split is done over the
edges where 80\% of the edges are in training and the remaining 20\% are divided amongst
heldout and test equally. By splitting, we mean that to obtain the training set we
randomly omit 20\% of the edges from the graph and keep only the remaining 80\%. The 
edges omitted from the training set are equally divided into the heldout and test sets. 
%For the two cancer datasets we only predict non-zero edge weights whereas for
%the Stack Overflow we predict zero as well as non-zero 
%edge weights.
Figure \ref{fig:EdgeDistribution} shows the distribution of edge weights in
the Cancer and Stack Overflow datasets. 
%We choose Stack Overflow to predict
%zero weights since it has large number of edges with very low weights,
%predominantly weight one. 
Predicting zero as well as non-zero edge weights
demonstrates that the model is versatile and can predict a wide range of 
edge-weights. Hence in addition to 20\% of the total
non-zero edges, we randomly sample an equal number of zero edges from the graph for
the held and test splits of all three datasets.
The optimization objective for learning is a lower bound 
on the log-likelihood (defined in the appendix)
%equation~\ref{eqn:VarLowerBound}.

A link prediction task is incorporated to demonstrate the model's effectiveness.
It is a standard task in the area of graph clustering and social networks in
particular. Researchers have
used it in the past to demonstrate their model's learning 
ability~\cite{Nallapati:2008:JLT:1401890.1401957}.   
The link prediction task works as an important validation of our model. If the
proposed model performs better than its individual parts then it can be safely
concluded that it extracts important patterns from each of its building
blocks. Moreover it adds validity to the qualitative analysis of the results. 

\vspace*{-0.5\baselineskip}
\paragraph{Link prediction} 
  
 We predict the edge weight of the edges
present in the test set. The predicted edge, $\hat{Y}_{t,u,v}$, between users $u$ and
$v$ in thread $t$ is  defined as 
\vspace*{-1\baselineskip}
\small
\begin{align}
	\hat{Y}_{t,u,v} &= \pi^T_uB\pi_v\label{eqn:prediction}.\\
				B&=\nu.*\lambda\label{eqn:blockMat}
\end{align}
\normalsize
and the prediction error is the $rmse$, defined in equation
\ref{eqn:rmse}, given predicted edge
$\hat{Y}_{t,u,v}$ and the actual edge $Y_{t,u,v}$.
\vspace*{-0.5\baselineskip}
\small
\begin{equation}
	rmse=\sqrt{\frac{\sum_{(u,v)\in E}(\hat{Y}_{t,u,v}-Y_{t,u,v})^2}{|E|}}.
	\label{eqn:rmse}
\end{equation}
\normalsize

\begin{figure}
\begin{center}
\includegraphics[height=6cm,width=9cm]{figs/topic_variations.pdf}
\end{center}
\vspace*{-2\baselineskip}
\caption{\small{Number of local variations in topic proportion on a per user per thread
level. The axis is percentage variation (from 10 to 90 percent).}}
\label{fig:localTopicVariations}
\vspace*{-1.5\baselineskip}
\end{figure}

\begin{figure*}
\begin{center}
\includegraphics[height=7cm,width=18cm]{SimilarityMatTSAnnotated.png}
\end{center}
\vspace*{-2\baselineskip}
\caption{Adjacency matrix of users sorted by community index. The left side is 
created using MMSB and the right side using our model with the 
users' dominant role as the community index over the TS dataset. 
Our model is able to correctly cluster 3K
additional users that MMSB doesn't assign any dominant community (or role) and
discovers a new community (role 6).}
\label{fig:similarityMatTS}
\vspace*{-1\baselineskip}
\end{figure*}
The summation is over the edge set $E$ in the test (or heldout) set. The block matrix
$B$ described in equation~\ref{eqn:blockMat} is well-defined for both MMSB and
the proposed model. Hence the prediction is obtained for the active network modelling 
both without
LDA (just MMSB component) and with LDA. We create an artificial
weighted identity matrix for LDA $\hat{B}=m*I$. It is a diagonal matrix with
all element values $m$. For every user $u$ and every thread $t$ the topics discovered over the
posts of $u$ in $t$ are used as the vector $\pi_u$ in
equation~\ref{eqn:prediction} for prediction. A diagonal $B$ is desirable in
block models as it provides clean separation among the clusters
obtained~\cite{Airoldi:2008:MMS:1390681.1442798}.
The value of $m$ is tuned over the heldout set. 
The MF model is the normal matrix factorization model~\cite{Gemulla:2011:LMF}. 
We obtain the 
matrix factors on the training set and use them to predict the edge weight between
users on the test and heldout sets. It does not use the text part of the data.
The BoW model is a simple bag-of-words model that takes each user $u$'s
aggregated post across the forum and constructs a vector $V_u$ of words. The 
edge weight $w_{u,v}$ predicted between users $u$ and $v$ is 
\vspace*{-0.5\baselineskip}
\small
\begin{align}
w_{u,v}=cosine\_sim(V_u,V_v)*b
\label{eqn:bow}
\end{align}
\normalsize
where $cosine\_sim(x,y)$ is the cosine similarity 
between vectors $x$ and $y$ and $b$ is tuned
over the heldout set. We define a naive baseline that
always predicts the average weight ($\bar{Y}$) of all the edges
in the heldout or test set.
\vspace*{-0.5\baselineskip}
\small
\begin{equation}
	rmse_{baseline}=\sqrt{\frac{\sum_{(u,v)\in E}(\bar{Y}_{t,u,v}-Y_{t,u,v})^2}{|E|}}.
\end{equation}
\normalsize

\vspace*{-0.5\baselineskip}
\paragraph{Community Discovery}
Besides providing qualitative insights into the communities dicovered
we also provide a quantitative evaluation of the communities. We use 
$\rho$, a variation of the Davies-Bouldin index~\cite{Davies:1979} 
\vspace*{-0.5\baselineskip}
\small
\begin{align}
	\rho=\frac{1}{K} \sum_{c=1}^{c=K}\frac{d_{c,in}}{d_{c,out}}
\end{align}
\normalsize
where $d_{c,in}=\frac{\sum_{u,v\in c}Y_{u,v}}{|E_{in}|}$ is the average weight of the 
edges of all user pairs inside the community $c$ and $|E_{in}|$ is the number of 
all such edges. $d_{c,out}=\frac{\sum_{u\notin c,v\in c}Y_{u,v}}{|E_{out}|}$ is the 
average weight of the edges of user pairs where one of the users lies in $c$ and the other 
outside $c$, and $|E_{out}|$ is the number of such edges. 
$\rho$ is the average ratio of the community densities and measures 
the compactness of the community.
We compare our model with simple MMSB and Graclus, a scalable spectral clustering
algorithm~\cite{Dhillon:2005}

\vspace*{-0.5\baselineskip}
\paragraph{Parameter tuning}
We tune our parameters $\eta, \kappa,
\theta, \alpha$, and $K$ (number of communities) over the heldout set.
$\omega$, the parameter to balance the contribution of the text side to the 
network side is tuned over the
heldout set. It is used in the local variational update of
$\phi$ (equation~\ref{eqn:phiDelta}).
Equation~\ref{eqn:phiDelta} contains a summation term over all the tokens
$\sum_{i=1}^{N_{T_p}}\chi_i$ in the per user per thread document and if  not
balanced by $\omega$ will dominate the rest of the terms. The constant $\epsilon$ used in
equation~\ref{eqn:phiDelta} and~\ref{eqn:chiDelta} is a smoothing constant and
is fixed at a low value. The six quantities, $\alpha, \omega, \theta, \kappa,
\eta$ and $K$ are tuned in that sequence. $\alpha$ is tuned first keeping the rest
constant, then $\omega$ and so on, where each parameter uses
the values of the previously tuned parameters. Table~\ref{tab:tunedParameters}
shows the final values of all the parameters. Figure~\ref{fig:finalLLheld} 
shows a plot of the tuned log-likelihood over the three
datasets against time. UM, being the smallest of the two, takes
the least amount of time. Tuning for the parameters of MF, BoW, Graclus and LDA
is done similarly on the heldout set.

\paragraph{System details} The machine used in all the experiments in this paper
is ``Intel(R) Xeon(R) CPU E5-2450 0 @ 2.10GHz'' 16 corewith 8GBs of
RAM per core. The operating system is Linux 2.6.32 x86\_64. 

\subsection{Results}
\label{sec:results}

\begin{table}
\begin{center} 
\begin{tabular}{p{1cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|}
  & UM held & UM test & TS held & TS test & SO held & SO test \\\hline
Our Model & \textbf{1.213} &\textbf{1.194} &\textbf{2.562} & \textbf{2.548} & 
\textbf{0.306}& \textbf{0.311} \\\hline 
MMSB &1.450& 1.502 & 2.984& 2.881 &0.421 & 0.434 \\\hline 
LDA &1.793& 1.731	&3.725 & 3.762 &0.466 & 0.479\\\hline
MF & 1.412 & 1.468 & 2.925 & 2.905 & 0.413 & 0.420 \\\hline
BoW &1.834 & 1.851 & 4.012 & 4.105 & 0.497 & 0.491 \\\hline
Baseline &1.886& 1.983 &4.504 &4.417&0.502& 0.509\\\hline
\end{tabular}
\vspace*{-0.5\baselineskip}
\caption{\small{Link prediction results over the three datsets}}
\label{tab:predictionResults}
\end{center}
\vspace*{-0.5\baselineskip}
\end{table}

\begin{table}
\begin{center} 
\begin{tabular}{p{1cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|}
  & UM held & UM test & TS held & TS test & SO held & SO test \\\hline
Our Model & \textbf{1.963} &\textbf{1.91} &\textbf{1.932} & \textbf{1.935} & 
\textbf{1.898}& \textbf{1.915} \\\hline 
MMSB &1.421& 1.417 & 1.443& 1.416 &1.387 & 1.404 \\\hline 
Graclus &1.598& 1.531	&1.460 & 1.457 &1.461 & 1.438\\\hline
\end{tabular}
\vspace*{-0.5\baselineskip}
\caption{\small{Average density ratios $\rho$ of different community 
discovery models over three datasets}}
\label{tab:rhoResults}
\end{center}
\vspace*{-1\baselineskip}
\end{table}

\begin{figure}
\centering
\hspace*{-1.5\baselineskip}
\begin{tabular}{l|l}
%	\hline
	\includegraphics[width=0.5\linewidth]{figs/scalabilityUsers.pdf}&
	\includegraphics[width=0.5\linewidth]{figs/scalabilityThreads.pdf}\\
%\hline
\end{tabular}
\vspace*{-1\baselineskip}
\caption{\small{Time taken in hours with increasing number of users and threads}}
\label{fig:syntheticScalability}
\end{figure}
 
\vspace*{-0.5\baselineskip}
\paragraph{Link prediction} Table~\ref{tab:predictionResults} shows the link
prediction results on the heldout and test sets for the six prediction
models. The proposed approach to model thread 
level conversational roles outperforms
all of the other models. LDA performs poorer than MMSB since LDA does not 
explicitly model network information. MF performs slightly better than MMSB, 
whereas BoW performs worse than LDA and slightly better than the baseline. 
This means that the text topic similarity contains valuable information regarding 
the users.

\vspace*{-0.5\baselineskip}
\paragraph{Cancer dataset}
Figure~\ref{fig:localTopicVariations} shows the number of times the global role
of a user is different from the thread level role that he or she plays. It is 
interesting to see that the variation between global and thread-level role
assignment is high among all the datasets. A model that ignores these local vs
global dynamics tends to lose a lot of information.
Figure~\ref{fig:similarityMatTS} shows the plot of the user-by-user adjacency 
matrix for TS dataset. The users are sorted based on the community topic
(roles) assigned by the respective models (our model and MMSB). The number of
community topics are 10 and every user is assigned the dominant
community topic (role) in $\pi$: the community $k$ such that $\pi[k]\geq0.5$, 
i.e. the user has more than 50\% of chance of lying in the community $k$. A user 
is discarded if he or she
doesn't have the said dominant role. Thus in this figure user role and community are the same
as explained in section~\ref{sec:term}.
Our model is unable to assign a clear 
role to 3.3K users, whereas
MMSB is unable to do so for approximately 6.3K users out of 22K. Based on the topics assigned,
users are sorted and their adjacency matrix shows clean clustering along the
block diagonals.
As seen in figure~\ref{fig:similarityMatTS}, 
the combined model is able to effectively find the
primary roles (dominant communities) for the extra 3K users that the MMSB model is
unable to account for. Besides that, a new role (role 6) that is not accounted for by
MMSB or Graclus is discovered by the proposed model (figure~\ref{fig:similarityMatTS}).
Users that predominantly have role 6 on a global scale tend to vary their roles
often on a thread level, i.e. their topic probabilities change quite often. The
average change in topic probabilities per role per user-thread pair across the
10 roles discovered in TS is 30.6\%; for role 6 it is 41.5\% (highest of all the roles). 
This means
that this role is very dynamic and an active sub-network modeling helps here as 
it captures the
dynamism of this entity. As seen in figure~\ref{fig:similarityMatTS}, roles
4, 5, 6, 7 and 8 are the largest. 
Graclus discovers a similar set of communities as the MMSB model. It also misses
community 6. Table~\ref{tab:rhoResults} shows that Graclus communities 
are marginally more compact than MMSB, but our model has the most compact 
communities. This is due to the fact that our model 
also takes into account local variations
in the user roles that helps it find better boundaries for the communities.
%\abhi{I will also put here the coommunities discovered by spectral clustering
%and make a contrast. To contrats with the better communties discovered by our model
%I will use the within-cluster-weight/without-cluster-weight and show that our
%model is better.}
%Table~\ref{tab:top15WordsTS} corresponds to
%top 15 words corresponding to these roles. 
A detailed inspection reveals that role 4, role 7 and role 8 are
related to discussion regarding cancer where as role 5 is related to
conversations regarding spiritual and family matters.  But role 6 does not seem
to be related to any specific type of conversation. It is free-flowing and has
lots of nonspecific discussion threads where users play games,
discuss cancer stages, talk about religious beliefs, share stories etc. 
This tells us that there is a multitude of
discussions happening within this role with no specific matter at hand. 
Users who are
predominantly in this role tend to post across many discussion threads and
variety of conversation topics. This role is only detected by our model 
because it can take into account the dynamics of such a role.

\begin{figure*}
\centering
  \includegraphics[width=1\linewidth]{piSOAnnotated.png}
\vspace*{-2\baselineskip}
\caption{\small{The 20 roles assigned to users in the Stack Overflow
dataset. The numbers at the vertex are the role numbers. Due to the large
number of roles we visualize them five at a time starting 
with first five then second five and so on.
We can see that the roles are separated cleanly and clustered around the
pentagon corners.}}
\label{fig:SOclusters}
\vspace*{-1\baselineskip}
\end{figure*}

\begin{figure}
\centering
  \includegraphics[width=1\linewidth]{figs/stability.pdf}
\vspace*{-2\baselineskip}
\caption{\small{RMSE vs $\alpha$ and $\eta$ for the synthetic dataset. The X-axis is
$\alpha$ or $\eta$ values and the Y-axis is the RMSE of the recovered $\pi$}}
\label{fig:syntheticRMSE}
\vspace*{-1\baselineskip}
\end{figure}
 
 
%\begin{table}
%\begin{center} 
%\begin{tabular}{c|c|c|c|c}
%Topic 4  & Topic 5 & Topic 6 & Topic 7 & Topic 8 \\\hline
%side            &same   &their  &surgeon        &radiat \\\hline
%test            &life   &live   &everi  &anoth\\\hline
%took            &tell   &happi  &found  &doctor\\\hline
%away            &mani   &mayb   &down   &problem\\\hline
%left            &famili &sorri  &alway  &pleas\\\hline
%support         &prayer &best   &while  &person\\\hline
%doesn           &though &check  &home   &kind\\\hline
%seem            &ladi   &news   &bodi   &soon\\\hline
%move            &until  &question       &these  &each\\\hline
%almost          &wish   &dure   &bone   &hard\\\hline
%scan            &someon &deal   &mean   &might\\\hline
%idea            &under  &case   &came   &medic\\\hline
%studi           &felt   &mind   &posit  &herceptin\\\hline
%guess           &where  &seem   &drug   &share\\\hline
%diseas          &nurs   &haven  &send   &free\\\hline
%\end{tabular}
%\caption{top 15 words for topics corresponding to top 5 biggest role in
%TS.}
%\label{tab:top15WordsTS}
%\end{center}
%\end{table}

%   \comment{dig in
% detail as to what this cluster is and show it in the context of local vs global roles}

%  \comment{Put interpretations of the clusters, UserName and
% ThreadStarter}\\
% \comment{Put the top 10 words in each topics}\\
% \comment{Put Block matrix to demonstrate better resolution of cluster}\\
% \comment{Put nice visualization for some per user-thread topic proportions.}\\

\vspace*{-0.5\baselineskip}
\paragraph{Stack Overflow} 
The optimal topic number for the SO dataset is 20 as noted in
table~\ref{tab:tunedParameters} and the number of users is 1.13 million. It is
difficult to visualize a user-user adjacency matrix of this size. The 20
topic set is divided into four sets of five. Topics 1 to 5
form set one, topics 6 to 10 form set two and so on. Every user's role is
visualized by projecting user's $\pi$ over a pentagon as shown in
figure~\ref{fig:SOclusters}. The projection uses both position and
color to show values of community-topic $\pi$ for each user.
Every user $u$ is displayed as a circle (vertex) $v_u$ in the figure where the
size of the circle is the node degree of $v_u$ and position of $v_u$ is equal to
a convex combination of the five pentagon corner coordinates $(x, y)$ that are weighted by the
elements of $\pi_u$. Hence circles $v_u$ at the pentagon's corners
represent $\pi$'s that have a dominating community  in the 5
community-topics chosen, while circles on the lines connecting the corners 
represent $\pi$'s with
mixed-membership in at least 2 communities (as only a partial $\pi$ vector is
used in each sub-graph).
All other circles represent $\pi$'s with mixed-membership in $\geq 3$
communities.
Each circle $v_u$'s color is also a $\pi$-weighted convex
combination of the RGB values of 5 colors: blue, green, red, cyan and purple. This
color coding helps distinguish between vertices with 2 versus 3 or more
communities. We observe a big black circle at the back ground of every plot.
This circle represents the user with id 22656
\footnote{\url{http://stackoverflow.com/users/22656/jon-skeet}} that 
has the highest node-degree of 25,220 in the SO dataset. 
This user has the highest all-time reputation on
Stack Overflow and tends to take part in myriads of question answering threads.
Hence he is rightly picked up by the model to be in the middle of all the roles. 

Figure~\ref{fig:SOclusters} has cleanly separated communities
where the nodes are clustered
around the pentagon vertices. This indicates that the model is able to find
primary roles for most of the users in the forum.  
Table~\ref{fig:localTopicVariations} shows that there is significant amount of
variation with respect to the global role of a user at a thread level. 
Modeling this variation helps our model in
getting better clusters as compared to simple MMSB and Graclus; 
this fact is apparent from
the $\rho$'s obtained, table~\ref{tab:rhoResults}. 
While Graclus's $\rho$ is marginally better than MMSB, our model outperforms
it by a significant margin. 
In terms of new communities discovered, we again observe a similar pattern
as in the case of the Cancer dataset where roles that have too much diversity 
are overlooked by MMSB and Graclus. 
Roles 1, 2 and 3 are related to discussions about a single
subject matter: database 
conversations, general coding and Android, respectively. 
By contrast, role 4 is diverse and users here discuss multiple 
subject matters: 
online blogs, Java, C++ multi-threading, Unix programming and browsing APIs. 
Role 5 and role 6 are associated with server/client-side browser-based 
coding and general coding, respectively. Role 7, like role 4, is diverse and 
conversations here are related to JSON, Python scripts, functional programing and
Clojure. MMSB and Graclus both overlook roles 4 and 7, whereas our model
discovers them as separate communities.

%\abhi{I will put here the communties discovered by simple MMSB 
%and Spectral Clutering and will make a point that our model discover 
%all the communties discoevred by these plus more as is the case for Cancer
%dataset. To contrats with the better communties discovered by our model
%I will use the within-cluster-weight/without-cluster-weight and show that our
%model is better.}


%\begin{table}
%\begin{center} 
%% \resizebox{\linewidth}{!}{
%\begin{tabular}{c|c|c|c|c|c}
%Topic 1  & Topic 2 & Topic 3 & Topic 4 & Topic 5 & Topic 6 \\\hline
%public  &code   &function       &that   &name   &array \\\hline
%valu    &should &also   &view   &method &more \\\hline
%chang   &your   &then   &thread &time   &what \\\hline
%when    &user   &object &into   &document       &system \\\hline
%event   &properti       &creat  &control        &line   &about \\\hline
%would   &blockquot      &follow &current        &element        &sure \\\hline
%list    &just   &form   &link   &differ &post \\\hline
%databas &field  &implement      &oper   &would  &each \\\hline
%there   &class  &valu   &question       &defin  &imag \\\hline
%issu    &overrid        &server &length &file   &class \\\hline
%path    &main   &veri   &creat  &result &paramet \\\hline
%display &result &string &each   &where  &applic \\\hline
%like    &size   &start  &result &more   &just \\\hline
%order   &import &java   &blog   &know   &local \\\hline
%save    &project        &android        &featur &browser        &specif \\\hline
%\end{tabular}%}
%\caption{top 15 words for word topics corresponding to first 6 role clusters in
%SO.}
%\label{tab:top15WordsSO}
%\end{center}
%\end{table}

%\abhi{Some text topic interpretation can be there but it could be removed
%in case of space crunch.}
Our model is not only better at discovering diverse roles, but also roles 
that predominantly belong to one category but are obscured at times. 
If the user deviates from his or her dominant role
for a short while then Graclus and MMSB have difficulty assigning
a predominant role. 
Take the user 20860 in the SO dataset. 20860 is globally assigned 
role 1 as the dominant role, but he also takes part in coding-related 
discussions sometimes. For example, 
\small
\begin{itemize}
  \item \textit{Because the join() method is in 
the string class, instead of the list class?
I agree it looks funny.}  
\item \textit{The simplest solution is to use
shell\_exec() to run the mysql client with the SQL script as input. 
This might run a little slower because it has to fork, but you can write 
the code in a couple of minutes and then get back to working on something useful. 
Writing a PHP script to run any SQL script could take you weeks\ldots.}
\end{itemize}
\normalsize
But in most of the cases, he participates in threads that are 
related to databases and this fact is picked up by our
model. It assigns him predominantly (>0.8) a database role 
even though he sometimes visits 
more general software and coding threads. 
MMSB, on the other hand, assigns him 30\%
database (role 1), 30\% general coding (role 2) and the 
rest is distributed among the
remaining 18 roles. Similarly, Graclus assigns the user role 1 as 35\%, role 2
as 25\% and the remainder is distributed almost equally 
to the rest of the roles. 
The model picks up other similar cases for which it is able
to successfully distinguish (compared to MMSB and Graclus) 
between users' global and local
roles even though they are dynamic in nature.
% E.g.
% user\ldots~\comment{Write about the top 20 and bottom 20 users with most and
% least variations accross threads }. 

% \comment{Put Block matrix to demonstrate better resolution of cluster}\\
% \comment{Put nice visualization for some per user-thread topic proportions.}\\

\vspace*{-0.5\baselineskip}
\paragraph{Synthetic dataset: stability and scalability}
Figure~\ref{fig:syntheticRMSE} gives the RMSE of the model for the recovery of
community topic $\pi$ over the synthetic dataset. From the graph, higher values
of the parameters make it harder to recover the $\pi$ values. For this experiment,
we fix the number of communities (K) at 5 and vary $\alpha$ or $\eta$ while fixing the other at
0.01. The other priors such as $\kappa, \theta,
\omega$, etc. are fixed at the values used to generate the dataset.  
It is apparent that the RMSE is more sensitive towards $\alpha$ values and
recovers them well compared to $\eta$. The results are averaged over 20 random
runs of the experiment for the given values of $\alpha$ and $\eta$. The RMSE
achieved for lower values of priors $\alpha$ and $\eta$ is very promising as
it means that the estimate is very good for sparse
priors for this model given sufficient data. 
Figure~\ref{fig:syntheticScalability} shows the scalability plot for two cases:
1) varying the number of users with a fixed number of threads at 100, and 2) 
varying the number of 
threads with a fixed number of users at 1000. We fix 
$\alpha=0.01$, $\eta=0.01$ and keep $\pi$ sparse.
In figure~\ref{fig:syntheticScalability}, for both cases 
the PSSV inference scales sub-linearly with an increasing number of threads or users. 
This implies that it is highly scalable in users and threads. A closer look
into the figure tells that increasing users has more effect on the time complexity 
than increasing threads. This is due to the fact that runtime is 
quadratic 
in the number of users ($O(users^2)$), but linear in number of threads ($O(threads)$)

%\abhi{I will put in more time plots on paralleization for synthetic data with 
%varying number of users, threads, topics, words. Also plots from section
%4 about scalability will have to connected with this section.}

\section{Related Work}

Ho et al.
\cite{Ho:2012:DHT:2187836.2187936} provide TopicBlock, a model 
that combines text and
network data in an efficient way for building a taxonomy for a corpus.
Nallapati et al. \cite{Nallapati:2008:JLT:1401890.1401957}
combine LDA and MMSB to obtain 
Pairwise-Link-LDA and Link-PLSA-LDA models that assign membership 
probabilities to documents for a set of bins (or classes).
These works 
combine MMSB and LDA to bring network and text together in a way
similar to our work, although for a tagential task. 
For simultaneously modeling topics in bilingual corpora, Smet et al.
\cite{Smet:2011:KTA:2017863.2017915} propose the Bi-LDA model that generates
topics in the target languages for paired documents in these languages.
They propose a latent space model that combines two LDA models 
where the end-goal is to classify any document into one of the
obtained set of topics. All the above works combine basic latent space modeling
blocks in interesting ways to provide useful results.  

White et al.\cite{ICWSM124638} propose a mixed-membership model that obtains
membership probabilities for discussion forum users for each statistic
(in- and out-degrees, initiation rate and reciprocity) in various profiles. It
then 
clusters the users into ``extreme profiles'' for user role-identification
and clustering based on roles in online communities. However, this approach
doesn't take into account the text used by users.
Sachan et
al.~\cite{Sachan:2012:UCI:2187836.2187882} provide a model for community
discovery that combines network edges with hashtags and other heterogeneous data
and use it to discover communities in Twitter and the Enron email dataset. But they
don't take the structure of the interaction into account. Additionally, they are 
not scalable; their largest dataset has less than 8,500 users.
For modeling the behavioral aspects of entities and
discovering communities in social networks, several game-theoretic approaches
have been proposed (Chen et al. \cite{Chen:2010:GFI:1842547.1842566}, Yadati and
Narayanam \cite{Yadati:2011:GTM:1963192.1963316}). However, none of them use it for
forum data containing both network and text. 

Zhu et
al.~\cite{Zhu:getoor:MMSB-text} combine MMSB and text for link prediction and
scale it to 44K links.
Ho et al.~\cite{HoYX12} provide  unique triangulated sampling schemes for scaling
mixed membership stochastic block models~\cite{Airoldi:2008:MMS:1390681.1442798} to
the order of hundreds of thousands of users. Prem et
al.~\cite{conf/nips/GopalanMGFB12} use stochastic variational inference 
coupled with sub-sampling techniques to
scale MMSB-like models to hundreds of thousands of users.
However, none of the works above address the sub-network dynamics of thread-based
discussion in online forums. Our work is unique in this context; it
brings user role modeling in online social networks closer to the
ground realities of online forum interactions.

Active sub-network modelling has been used recently to model gene interaction 
networks. Lichtenstein et al.~\cite{Lichtenstein:Charleston}
combine gene expression data with network topology to provide bio-molecular 
sub-networks, though their approach is not scalable as they use simple EM for
their inference. We leverage the scalable aspects of
SVI~\cite{Hoffman:2013:SVI} to combine MMSB (network topology) with LDA (post
contents) in a specific graphical structure (thread structure in the forum) to
obtain a highly scalable active sub-network discovery model.

Matrix factorization and spectral learning based approaches are some of the
other popular schemes for modelling user networks and content. In recent
past both approaches have been made scalable to a million order node size graph
~ \cite{Gemulla:2011:LMF,Dhillon:2005}. But these methods are unable to incorporate 
the rich structure that a probabilistic modeling based method 
can take into account as shown by the empirical results in the 
previous section.



\section{Discussion and Future Work}
The  proposed model relies on the fact that forum users have dynamic role
assignments in online discussions and leveraging this fact helps increase
prediction performance as well as understand forum activity. The model
performs very well in its prediction tasks. It outperforms all the other methods
over all the datsets by a large margin. The model is scalable and is able to run
on social network datasets of unprecedented sizes. There is no past
research work that scales forum contents to more than one million users 
and 9 million posts. 

The idea that active subnetwork is useful in modeling online forums is
demonstrated qualitatively and quantitatively. Quantitatively it provides better
prediction performance and qualitatively it captures the dynamics of user
roles in forums. This dynamism helps us find new user roles that may have
been missed by state-of-the art clustering approaches.
From the synthetic experiments it is observed that the model
recovers its parameters with high likelihood with sparse priors. This works to
its advantage for scalable learning as big datasets tend to be sparse.

The scalability aspects of the inference scheme proposed here are worth noting.
Besides the multi-core and stochastic sub-sampling components of the proposed
inference, the use of Poisson to model the edge weights has enabled us to 
ignore zero-edges if need be. This reduces the amount of work needed to
learn the network parameters. The learned network is at par with the state-of-the art
inference schemes as demonstrated in the prediction tasks.   

One aspect to explore in the future is to combine multiple types of links in the
network. For example, in many online forums users explicitly friend or
follow other users or join forum groups. 
All these relations can be modeled as a graph. It is worth finding out
how important modeling active sub-network is in such a case. It is
possible that various types of links might reinforce each other in learning
the parameters and thus will obviate the need to model a computationally
costly sub-network aspect. As we saw in figure~\ref{fig:syntheticRMSE}, 
sparsity helps, but how much sparser we can get before we start getting poor results
needs some exploration.

Also
seen in figure ~\ref{fig:syntheticRMSE}, the model recovers the
community-topic parameters with very high likelihood for lower values of model
priors $\alpha$ and $\tau$. If this is a general attribute of active sub-network
models then it can be leveraged for sparse learning. Although in the case
of large online forums modeling active sub-networks is computationally
challenging and costly, the sparsity aspects of active sub-networks might help
reduce the computation costs.


\bibliography{mybib}
\bibliographystyle{plain}
\appendix
\input{appendix}

\end{document}
